# Blind Reviews Did not Work for Us

We recognize the open CFP on Github is a big change from our submission process of the past two years and a change from blind reviews and community voting, so it’s important for us to be transparent about our reasons and our process.

It isn't that blind reveiw processes don't work, but it didn't work for us. In 2014, we fully anonymized our submissions—any references to gender, online work, employer, were removed from the abstracts. The speaker team reviewed and selected 1/3 of the talks for voting by a community review board comprised of active Sass community members who had not submitted talks or were not attending the conference as well as past speakers and community members. Results from the community board vote were tallied. The rankings were not the sole determinating factor, but they did play a part in our final selection. Then we uncovered the names.

We were utterly confused. We thought this process was supposed to help us lead to a more diverse selection. Instead, we had very few women and no diversity across geners. We went back to the original submissions. The problem isn't with blind reviews, it was with the submission pool. Of the total submissions less than 5% were talks by women. If you don’t have a diverse pool of proposals to select from, it is impossible to expecte a diverse lineup.

It was a clear moment for us. This meant more than just putting out a CFP, offering office hours, and getting those championing for opportunities for diversity to retweet and post our proposal. From our conversations with conference organizers, such as Dan Denney and Karolin Szur, both of whom are doing an exemplary job at balanced submissions events, we knew we need to dialouge and reach out directly. 

> “I’m not giving priority to [diverse speakers] although I go out of my way to encourage them to submit.” — Karolina Szczur, CSSConfOakland

### Community voting will well intentioned is also baised

As a conference attendee, you’ve probably filled out the incredibly long voting surveys. We don’t believe it’s the attendees’ job to pick talks, nor do we think those long surveys really get the job done. How many of you go through and carefully vote on _every single proposal_? How many of you give up after a page or two—and how badly does that skew the voting? 

In 2014, we introduced a community review board to review 35 talks. While this was a good exercise for us to go through and beneficial within last year's context, we decided to completely scrap voting surveys in turn for fully open submissions. 

We hope you'll offer your input by using Github's commenting system. We encourage you to ask questions and to let us know what you think of the submissions. If you've seen the person speak and enjoyed their work, let us know. We are listening and we will be actively engaging and dialouging with you throughout the open call.  

